{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Reviews - Sentiment Analysis and Machine Learning\n",
    "\n",
    "*Ville Kylmämaa, Joona Holappa, Miiro Kuosmanen, Anssi Valjakka*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T18:45:09.740478Z",
     "start_time": "2022-04-15T18:45:07.101021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python built-in modules\n",
    "import sys\n",
    "import os.path\n",
    "import json\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "\n",
    "# %pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "# %pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "# %pip install nltk\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# %pip install scipy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# %pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# %pip install empath\n",
    "from empath import Empath\n",
    "\n",
    "# %pip install wordcloud\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T18:45:18.492995Z",
     "start_time": "2022-04-15T18:45:09.743821Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read restaurant reviews to pandas dataframe format\n",
    "reviews_df = pd.read_csv('Restaurant_Reviews.tsv', sep='\\t')\n",
    "print(reviews_df)\n",
    "\n",
    "# Separate lists for Review and Liked columns\n",
    "review_column = reviews_df[\"Review\"]\n",
    "liked_column = reviews_df[\"Liked\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nThe number of dislike (0) reviews in the data:\\n{liked_column.tolist().count(0)}\")\n",
    "print(f\"\\nThe number of like (1) reviews in the data:\\n{liked_column.tolist().count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 1\n",
    "\n",
    "Use initially SentiStrength SentiStrength - sentiment strength detection in short texts - sentiment analysis,\n",
    "opinion mining (http://sentistrength.wlv.ac.uk/) implementation of sentiment, which provides negative and positive\n",
    "sentiment score, compute Pearson correlation between this constructed sentiment polarity and the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize SentiStrength\n",
    "# http://sentistrength.wlv.ac.uk/\n",
    "senti_strength_path = \"./sentistrength/SentiStrength.jar\"\n",
    "language_folder_path = \"./sentistrength/SentiStrength_Data/\"\n",
    "\n",
    "# Check that the files exist in the given paths\n",
    "if not os.path.isfile(senti_strength_path):\n",
    "    print(\"SentiStrength not found at: \", senti_strength_path)\n",
    "if not os.path.isdir(language_folder_path):\n",
    "    print(\"SentiStrength data folder not found at: \", language_folder_path)\n",
    "\n",
    "# Returns SentiStrength sentiment score for the given string\n",
    "def rate_sentiment(sentiString):\n",
    "    # Open a subprocess using shlex to get the command line string into the correct args list format.\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + senti_strength_path + \"' stdin sentidata '\" + language_folder_path + \"'\"),\n",
    "                         stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # Communicate via stdin the string to be rated. Note that all spaces are replaced with +.\n",
    "    # Can't send string in Python 3, must send bytes.\n",
    "    b = bytes(sentiString.replace(\" \",\"+\"), 'utf-8')\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")\n",
    "    # Remove the tab spacing between the positive and negative ratings. e.g. \"1    -5\" -> \"1 -5\"\n",
    "    stdout_text = stdout_text.rstrip().replace(\"\\t\",\" \")\n",
    "    return stdout_text\n",
    "\n",
    "# Convert the score from 1 -5 to binary: 0 for dislike, 1 for like\n",
    "# \"Neutral\" score, for example -2 2, is cast as 0\n",
    "def score_to_binary(score_original):\n",
    "    binary_score = score_original.split(' ')\n",
    "    binary_score = list(binary_score)\n",
    "    binary_score = [1 if int(binary_score[i]) > abs(int(binary_score[i+1])) else 0 for i in range(0, len(binary_score), 3)]\n",
    "    return binary_score[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SentiStrength usage example\n",
    "\n",
    "example_sentence = \"'What a lovely day!'\"\n",
    "\n",
    "rated = rate_sentiment(example_sentence)\n",
    "converted = score_to_binary(rated)\n",
    "\n",
    "print(\"\\nRating the sentence:\")\n",
    "print(example_sentence)\n",
    "\n",
    "print(\"\\nRated by SentiStrength:\")\n",
    "print(rated)\n",
    "\n",
    "print(\"\\nRating converted to 0 for dislike / 1 for like:\")\n",
    "print(converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect sentiments evaluated by SentiStrength for each review\n",
    "# ! Running this takes a few minutes (approximately 3-5min)\n",
    "\n",
    "sentistrength_sentiments = []\n",
    "\n",
    "for row in review_column:\n",
    "    rated_sentiment = rate_sentiment(row)\n",
    "    converted_sentiment = score_to_binary(rated_sentiment)\n",
    "    sentistrength_sentiments.append(converted_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Pearson correlation between the SentimentStrength evaluations and the liked column\n",
    "\n",
    "pearson_corr_coeff = np.corrcoef(sentistrength_sentiments, liked_column)\n",
    "\n",
    "print(f\"\\nPearson correlation coefficient matrix:\\n{pearson_corr_coeff}\")\n",
    "print(f\"\\nPearson correlation between textblob sentiment and review liked score:\\n{pearson_corr_coeff[1][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Repeat this process when considering the correlation of the positive class alone and the correlation of the negative class alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate overall cosine similarity\n",
    "cosine_similarity = cosine(sentistrength_sentiments, liked_column)\n",
    "print(\"\\nOverall cosine similarity:\", cosine_similarity)\n",
    "\n",
    "\n",
    "# Calculate cosine similarity of positive class alone\n",
    "positive_sentistrength_sentiments = []\n",
    "positive_corresponding_liked_column = []\n",
    "\n",
    "for index, rating in enumerate(sentistrength_sentiments):\n",
    "    if rating == 1:\n",
    "        positive_sentistrength_sentiments.append(sentistrength_sentiments[index])\n",
    "        positive_corresponding_liked_column.append(liked_column[index])\n",
    "\n",
    "positive_cosine_similarity = cosine(positive_sentistrength_sentiments, positive_corresponding_liked_column)\n",
    "print(\"Positive cosine similarity:\", positive_cosine_similarity)\n",
    "\n",
    "\n",
    "# Calculate cosine similarity of negative class alone\n",
    "negative_sentistrength_sentiments = []\n",
    "negative_corresponding_liked_column = []\n",
    "\n",
    "for index, rating in enumerate(sentistrength_sentiments):\n",
    "    if rating == 0:\n",
    "        # Add miniscule amount to the 0 vector to avoid division by 0 in the cosine similarity calculation\n",
    "        negative_sentistrength_sentiments.append(sentistrength_sentiments[index] + 0.00000000000000001)\n",
    "        negative_corresponding_liked_column.append(liked_column[index])\n",
    "\n",
    "negative_cosine_similarity = cosine(negative_sentistrength_sentiments, negative_corresponding_liked_column)\n",
    "print(\"Negative cosine similarity:\", negative_cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 3\n",
    "\n",
    "Now we want to test the correlation with respect to some stylistic aspects of the review. Write a script that estimate\n",
    " the number of personal pronouns and number of adjectives and number of adverbs using part-of-speech tagger of your\n",
    "  choice. Compute both the cosine similarity between each of the above attributes (number of pronouns, number of\n",
    "   adjectives, number of adverbs) and the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Printing utility functions\n",
    "def print_pearson_corr(first_thing_name, second_thing_name, pearson_corr_coeff):\n",
    "    print(f\"Pearson correlation between {first_thing_name} and {second_thing_name}:\\n{pearson_corr_coeff[1][0]}\\n\")\n",
    "\n",
    "def print_cosine_similarity(first_thing_name, second_thing_name, cosine_similarity):\n",
    "    print(f\"Cosine similarity between {first_thing_name} and {second_thing_name}:\\n{cosine_similarity}\\n\")\n",
    "\n",
    "\n",
    "pronouns_in_review = []\n",
    "adjectives_in_review = []\n",
    "adverbs_in_review = []\n",
    "\n",
    "# Count the pronouns, adjectives and adverbs in each review and add them to the corresponding lists\n",
    "for row in review_column:\n",
    "    pronoun_count = 0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "\n",
    "    for word in row.split(\" \"):\n",
    "        # Part of speech tag, pos_tag reference: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "        word_pos = pos_tag(word_tokenize(word))\n",
    "\n",
    "        if word_pos[0][1] == \"PRP\":\n",
    "            pronoun_count += 1\n",
    "        if word_pos[0][1] == \"JJ\" or word_pos[0][1] == \"JJR\" or word_pos[0][1] == \"JJS\":\n",
    "            adjective_count += 1\n",
    "        if word_pos[0][1] == \"RB\" or word_pos[0][1] == \"WBR\":\n",
    "            adverb_count += 1\n",
    "\n",
    "    pronouns_in_review.append(pronoun_count)\n",
    "    adjectives_in_review.append(adjective_count)\n",
    "    adverbs_in_review.append(adverb_count)\n",
    "\n",
    "\n",
    "print(\"\\n---PEARSON CORRELATIONS---\\n\")\n",
    "pearson_corr_coeff_pronoun = np.corrcoef(pronouns_in_review, liked_column)\n",
    "pearson_corr_coeff_adjective = np.corrcoef(adjectives_in_review, liked_column)\n",
    "pearson_corr_coeff_adverb = np.corrcoef(adverbs_in_review, liked_column)\n",
    "print_pearson_corr(\"number of PRONOUNS\", \"review liked score\", pearson_corr_coeff_pronoun)\n",
    "print_pearson_corr(\"number of ADJECTIVES\", \"review liked score\", pearson_corr_coeff_adjective)\n",
    "print_pearson_corr(\"number of ADVERBS\", \"review liked score\", pearson_corr_coeff_adverb)\n",
    "\n",
    "print(\"\\n---COSINE SIMILARITIES---\\n\")\n",
    "cosine_similarity_pronoun = cosine(pronouns_in_review, liked_column)\n",
    "cosine_similarity_adjective = cosine(adjectives_in_review, liked_column)\n",
    "cosine_similarity_adverb = cosine(adverbs_in_review, liked_column)\n",
    "print_cosine_similarity(\"number of PRONOUNS\", \"review liked score\", cosine_similarity_pronoun)\n",
    "print_cosine_similarity(\"number of ADJECTIVES\", \"review liked score\", cosine_similarity_adjective)\n",
    "print_cosine_similarity(\"number of ADVERBS\", \"review liked score\", cosine_similarity_adverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 4\n",
    "\n",
    "We want to test the hypothesis that the opinion of about the restaurant is constructed according to Price, Quality\n",
    " of food served in the restaurant, and friendly staff. Suggest a script that allows you to identify Review that are\n",
    "  more focused on price, quality of food, friendly staff. You may consider a set of keywords that are most suitable\n",
    "   to each category and then use simple string matching to match this effect. For each category, generate a binary\n",
    "    vector indicating whether the given review focuses on the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def any_keywords_in_string(keywords, row):\n",
    "    words = word_tokenize(row.lower())\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words] # Lemmatisation\n",
    "\n",
    "    if any(keyword in words for keyword in keywords):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "keyword_file = open(\"keywords.json\", \"r\")\n",
    "keywords = json.loads(keyword_file.read())\n",
    "keyword_file.close()\n",
    "\n",
    "price_focus = []\n",
    "quality_focus = []\n",
    "staff_focus = []\n",
    "\n",
    "for row in review_column:\n",
    "    price_focus.append(any_keywords_in_string(keywords[\"price\"], row))\n",
    "    quality_focus.append(any_keywords_in_string(keywords[\"quality\"], row))\n",
    "    staff_focus.append(any_keywords_in_string(keywords[\"staff\"], row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 5\n",
    "\n",
    "Estimate the correlation using Pearson correlation between each vector category and the data annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pearson_corr_coeff_price = np.corrcoef(price_focus, liked_column)\n",
    "pearson_corr_coeff_quality = np.corrcoef(quality_focus, liked_column)\n",
    "pearson_corr_coeff_staff = np.corrcoef(staff_focus, liked_column)\n",
    "\n",
    "print_pearson_corr(\"PRICE\",\"liked score\", pearson_corr_coeff_price)\n",
    "print_pearson_corr(\"QUALITY\",\"liked score\", pearson_corr_coeff_quality)\n",
    "print_pearson_corr(\"STAFF\", \"liked score\", pearson_corr_coeff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 6\n",
    "\n",
    "We want to revisit the construction of the categories in 4). Instead of string matching, use the semantic\n",
    "similarity in the following way. Calculate the Wu and Palmer similarity between “price” and the Review (using\n",
    "the sentence-to-sentence similarity as in labs), repeat this process for the other three categories by suggestion\n",
    "a representative keyword (s) that will be used to calculate sentence-to-sentence similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Task 6 TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 7\n",
    "\n",
    "We want to test another approach for computing the categories by using the empath categories embedding. For this\n",
    "purpose, re-visit the naming of the empath-categories in GitHub - Ejhfast/empath-client: analyze text with empath\n",
    "(https://github.com/Ejhfast/empath-client) and select those that might be linked to Price, Quality, Staff friendship.\n",
    "Write a code that allows you to determine appropriate categories from this embedding and then calculate the correlation\n",
    "score.  Alternative to manual scrutinization of the Empath categories, you may also generate an empath category\n",
    "embedding for the keyword “price”, “food quality”, “friendly staff”, and then compute cosine similarity between the\n",
    "Review embedding vector and each of the above four embedding vectors, so that the one that yields the highest similarity\n",
    "score will be considered as the one that best represents the underlined category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lexicon = Empath()\n",
    "\n",
    "print(\"\\nPrice category:\")\n",
    "lexicon.create_category(\"price\", keywords[\"price\"], size=50)\n",
    "\n",
    "print(\"\\nFood quality category:\")\n",
    "lexicon.create_category(\"quality\", keywords[\"quality\"], size=50)\n",
    "\n",
    "print(\"\\nStaff friendliness category:\")\n",
    "lexicon.create_category(\"staff\", keywords[\"staff\"], size=50)\n",
    "\n",
    "empath_price = []\n",
    "empath_quality = []\n",
    "empath_staff = []\n",
    "\n",
    "for row in review_column:\n",
    "    empath_price.append(lexicon.analyze(row, categories=[\"price\"], normalize=True).get(\"price\"))\n",
    "    empath_quality.append(lexicon.analyze(row, categories=[\"quality\"], normalize=True).get(\"quality\"))\n",
    "    empath_staff.append(lexicon.analyze(row, categories=[\"staff\"], normalize=True).get(\"staff\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n---EMPATH COSINE SIMILARITIES---\\n\")\n",
    "\n",
    "cosine_sim_empath_price = cosine(empath_price, liked_column)\n",
    "cosine_sim_empath_quality = cosine(empath_quality, liked_column)\n",
    "cosine_sim_empath_staff = cosine(empath_staff, liked_column)\n",
    "print_cosine_similarity(\"number of Empath PRICE\", \"review liked score\", cosine_sim_empath_price)\n",
    "print_cosine_similarity(\"number of Empath QUALITY\", \"review liked score\", cosine_sim_empath_quality)\n",
    "print_cosine_similarity(\"number of Empath STAFF\", \"review liked score\", cosine_sim_empath_staff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 8\n",
    "\n",
    "We want to further emphasize on misclassified reviews. For this purpose, concatenate all reviews for which the\n",
    "sentiment score is positive while the annotation is zero and those for which the sentiment is zero while the\n",
    "annotation is 1. Construct the Wordcloud of this dataset. Write a histogram showing the 10 most common wordings\n",
    "in this dataset. Comment on the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect misclassified ratings\n",
    "# ! Running this takes a few minutes (approximately 3-5min)\n",
    "\n",
    "misclassified_reviews = []\n",
    "\n",
    "for index, row in enumerate(review_column):\n",
    "    rated_sentiment = rate_sentiment(row)\n",
    "    converted_sentiment = score_to_binary(rated_sentiment)\n",
    "\n",
    "    if converted_sentiment != liked_column[index]:\n",
    "        misclassified_reviews.append([row, liked_column[index]])\n",
    "\n",
    "print(\"\\nNumber of misclassified ratings:\")\n",
    "print(len(misclassified_reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-processing: lemmatisation, turn all chars to lowercase, leave out stopwords and words containing numbers\n",
    "# Returns a list of tokens\n",
    "def pre_process(doc_string):\n",
    "    # Use stopwords from nltk\n",
    "    STOPWORDS = list(set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "    # Tokenize words and turn them to lowercase\n",
    "    words = word_tokenize(doc_string.lower())\n",
    "\n",
    "    # Lemmatisation\n",
    "    words = [WordNetLemmatizer().lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "    # Leave out stopwords and words containing numbers\n",
    "    words = [\n",
    "        word for word in words\n",
    "        if word.isalpha()\n",
    "        and word not in STOPWORDS\n",
    "    ]\n",
    "    return words\n",
    "\n",
    "\n",
    "# Pre-process the collected misclassified ratings\n",
    "misclassified_reviews_string = \"\"\n",
    "for review in misclassified_reviews:\n",
    "    misclassified_reviews_string += f\"{review[0]} \"\n",
    "misclassified_reviews_preprocessed = pre_process(misclassified_reviews_string)\n",
    "\n",
    "print(\"\\nNumber of word tokens after preprocessing:\")\n",
    "print(len(misclassified_reviews_preprocessed), \"\\n\")\n",
    "\n",
    "print(\"Word tokens after preprocessing (print truncated to first 50 elements):\")\n",
    "print(misclassified_reviews_preprocessed[:50], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect 10 most common words\n",
    "most_common_words = Counter(misclassified_reviews_preprocessed).most_common(10)\n",
    "\n",
    "# Parse the generated tuples into two different arrays\n",
    "words = [word for word, _ in most_common_words]\n",
    "counts = [counts for _, counts in most_common_words]\n",
    "\n",
    "# Plot the words and their frequency\n",
    "plt.bar(words, counts)\n",
    "plt.title(\"Histogram of the 10 most common words in misclassified reviews\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate word cloud of the misclassified reviews\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "    width=3000,\n",
    "    height=2000,\n",
    "    random_state=1,\n",
    "    background_color=\"#1f1f36\",\n",
    "    colormap=\"Blues\",\n",
    "    collocations=False,\n",
    ").generate(\" \".join(misclassified_reviews_preprocessed))\n",
    "\n",
    "print(\"\\nWord cloud of the misclassified reviews:\")\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 9.\n",
    "\n",
    "Now we would like to build a machine learning model for sentiment analysis that takes into account the ambiguous\n",
    "cases identified in 9). For this purpose, write and script and review the preprocessing and stopword list to not\n",
    "discard relevant information in the context of sentiment analysis (e.g., avoid discarding negation cues, adjectives\n",
    "that subsumes polarity and apostrophes, lower-case as capitalization brings emotion,..), then use TfIdfVectorizer\n",
    "with a maximum feature set of 500, minimum 2 repetition and no more than 60% of word repetition across sentences.\n",
    "Build this model for one dataset using randomly selected 70% training and 30% testing. Report the classification\n",
    "accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_review_column = [review[0] for review in misclassified_reviews]\n",
    "misclassified_liked_column = [review[1] for review in misclassified_reviews]\n",
    "\n",
    "# Lemmatisation is the only type of pre-processing used for the machine learning models.\n",
    "# Removing stopwords discards important negation cues. For example, the word \"not\" -> \"not good\" becomes \"good\".\n",
    "# Turning the words to lowercase is not used because capitalization brings emotion. For example, FULL CAPITALIZATION.\n",
    "misclassified_review_column = [WordNetLemmatizer().lemmatize(word) for word in misclassified_review_column]\n",
    "\n",
    "# Divide the reviews into training and test data\n",
    "review_train, review_test, liked_train, liked_test = train_test_split(\n",
    "    misclassified_review_column,\n",
    "    misclassified_liked_column,\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "# Use tf-idf vectorizer to fit and transform review training data\n",
    "TFIDF = TfidfVectorizer(max_features=500, max_df=0.60, min_df=2) \n",
    "tfidf_fit_trans_review = TFIDF.fit_transform(review_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support vector classification model\n",
    "svc_model = SVC()\n",
    "\n",
    "# Train the model with the review column tf-idf fitted and transformed training data, and liked column training data\n",
    "svc_model.fit(\n",
    "    tfidf_fit_trans_review,\n",
    "    liked_train\n",
    ")\n",
    "\n",
    "# Make predictions using the model\n",
    "svc_model_predictions = svc_model.predict(\n",
    "    TFIDF.transform(review_test)\n",
    ")\n",
    "\n",
    "# Get accuracy of the predictions\n",
    "svc_model_accuracy = accuracy_score(liked_test, svc_model_predictions)\n",
    "\n",
    "print(f\"\\nSupport vector classification model accuracy:\\n{svc_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic regression model\n",
    "log_reg_model = LogisticRegression()\n",
    "\n",
    "# Train the model with the review column tf-idf fitted and transformed training data, and liked column training data\n",
    "log_reg_model.fit(\n",
    "    tfidf_fit_trans_review,\n",
    "    liked_train\n",
    ")\n",
    "\n",
    "# Make predictions using the model\n",
    "log_reg_model_predictions = log_reg_model.predict(\n",
    "    TFIDF.transform(review_test)\n",
    ")\n",
    "\n",
    "# Get accuracy of the predictions\n",
    "log_reg_model_accuracy = accuracy_score(liked_test, log_reg_model_predictions)\n",
    "\n",
    "print(f\"\\nSupport vector classification model accuracy:\\n{log_reg_model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 10\n",
    "\n",
    "Use Glove embedding instead of TfidfVectorizer, see GloVe: Global Vectors for Word Representation\n",
    "(https://nlp.stanford.edu/projects/glove/). Use the Glove embedding as feature vectors and test the performance in\n",
    "the original data (30% test data) and report the classification accuracy on the other two datasets. Comment on the\n",
    "limitations of the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 11\n",
    "\n",
    "Identify appropriate literature to comment on your findings and methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Task 11 TODO\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c5a4743cda23ca51c4316defd6eef32baef9dc6029e48c24d82dfe2c9f06bc0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
